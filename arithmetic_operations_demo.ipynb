{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356e6314",
   "metadata": {},
   "source": [
    "# Tensor的算术运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea5291",
   "metadata": {},
   "source": [
    "注意：所有带了`_`的函数都是原地操作，会直接修改原张量的值。\n",
    "\n",
    "原地操作的函数通常会返回`None`，而不是新的张量。\n",
    "\n",
    "不推荐使用原地操作，因为它们可能会导致后续计算出错。\n",
    "\n",
    "原地操作更多地用于内存优化。\n",
    "\n",
    "对于原地操作，需要提前预见张量的类型，如果操作前后的类型不一致，会导致错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852ac7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5e234",
   "metadata": {},
   "source": [
    "## 四则运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41536de",
   "metadata": {},
   "source": [
    "### 加法运算——`add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d38e84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "b: tensor([[-0.7666, -1.6691, -0.3594],\n",
      "        [-0.6778, -0.6803,  0.1596]])\n",
      "a + b: tensor([[-0.7423, -1.6025, -0.5190],\n",
      "        [-1.4874, -0.5546,  0.0895]])\n",
      "a.add(b): tensor([[-0.7423, -1.6025, -0.5190],\n",
      "        [-1.4874, -0.5546,  0.0895]])\n",
      "torch.add(a, b): tensor([[-0.7423, -1.6025, -0.5190],\n",
      "        [-1.4874, -0.5546,  0.0895]])\n",
      "a.add_(b): tensor([[-0.7423, -1.6025, -0.5190],\n",
      "        [-1.4874, -0.5546,  0.0895]])\n",
      "a after a.add_(b): tensor([[-0.7423, -1.6025, -0.5190],\n",
      "        [-1.4874, -0.5546,  0.0895]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(2, 3)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a + b:\", a + b)\n",
    "print(\"a.add(b):\", a.add(b))\n",
    "print(\"torch.add(a, b):\", torch.add(a, b))\n",
    "print(\"a.add_(b):\", a.add_(b))\n",
    "print(\"a after a.add_(b):\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a2d43",
   "metadata": {},
   "source": [
    "### 减法运算——`sub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d9796c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a - b: tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "a.sub(b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "torch.sub(a, b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "a.sub_(b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "a after a.sub_(b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n"
     ]
    }
   ],
   "source": [
    "print(\"a - b:\", a - b)\n",
    "print(\"a.sub(b):\", a.sub(b))\n",
    "print(\"torch.sub(a, b):\", torch.sub(a, b))\n",
    "print(\"a.sub_(b):\", a.sub_(b))\n",
    "print(\"a after a.sub_(b):\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814e2ce",
   "metadata": {},
   "source": [
    "### 乘法运算——`mul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b52526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b: tensor([[-0.0186, -0.1112,  0.0574],\n",
      "        [ 0.5487, -0.0855, -0.0112]])\n",
      "a.mul(b): tensor([[-0.0186, -0.1112,  0.0574],\n",
      "        [ 0.5487, -0.0855, -0.0112]])\n",
      "torch.mul(a, b): tensor([[-0.0186, -0.1112,  0.0574],\n",
      "        [ 0.5487, -0.0855, -0.0112]])\n",
      "a.mul_(b): tensor([[-0.0186, -0.1112,  0.0574],\n",
      "        [ 0.5487, -0.0855, -0.0112]])\n",
      "a after a.mul_(b): tensor([[-0.0186, -0.1112,  0.0574],\n",
      "        [ 0.5487, -0.0855, -0.0112]])\n"
     ]
    }
   ],
   "source": [
    "print(\"a * b:\", a * b)\n",
    "print(\"a.mul(b):\", a.mul(b))\n",
    "print(\"torch.mul(a, b):\", torch.mul(a, b))\n",
    "print(\"a.mul_(b):\", a.mul_(b))\n",
    "print(\"a after a.mul_(b):\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366942a",
   "metadata": {},
   "source": [
    "### 除法运算——`div`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6627e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.div(b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "torch.div(a, b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "a.div_(b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n",
      "a after a.div_(b): tensor([[ 0.0242,  0.0666, -0.1596],\n",
      "        [-0.8096,  0.1257, -0.0701]])\n"
     ]
    }
   ],
   "source": [
    "print(\"a.div(b):\", a.div(b))\n",
    "print(\"torch.div(a, b):\", torch.div(a, b))\n",
    "print(\"a.div_(b):\", a.div_(b))\n",
    "print(\"a after a.div_(b):\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb641a",
   "metadata": {},
   "source": [
    "## 矩阵、高维Tensor乘法\n",
    "\n",
    "矩阵、高维Tensor乘法运算不存在`_`的原地操作。由于无法确定计算得到的张量形状和原本张量形状一致，所有的矩阵运算都是创建一个新的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb51969",
   "metadata": {},
   "source": [
    "### 矩阵乘法——`matmul`\n",
    "- `torch.matmul`是矩阵乘法的函数，等价于`@`运算符、`torch.mm`。\n",
    "- 如果`a`是一个mxn的矩阵，则`b`是一个nxp的矩阵，那么`c`将是一个mxp的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6073359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[ 0.6085, -0.9408, -1.3502],\n",
      "        [ 1.4057, -0.8519,  1.1425]])\n",
      "b: tensor([[ 0.3712,  1.0338],\n",
      "        [-0.0474,  0.2137],\n",
      "        [-1.0411, -0.1239]])\n",
      "a @ b: tensor([[ 1.6762,  0.5953],\n",
      "        [-0.6274,  1.1296]])\n",
      "torch.matmul(a, b): tensor([[ 1.6762,  0.5953],\n",
      "        [-0.6274,  1.1296]])\n",
      "torch.mm(a, b): tensor([[ 1.6762,  0.5953],\n",
      "        [-0.6274,  1.1296]])\n",
      "a.matmul(b): tensor([[ 1.6762,  0.5953],\n",
      "        [-0.6274,  1.1296]])\n",
      "a.mm(b): tensor([[ 1.6762,  0.5953],\n",
      "        [-0.6274,  1.1296]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(3, 2)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a @ b:\", a @ b)\n",
    "print(\"torch.matmul(a, b):\", torch.matmul(a, b))\n",
    "print(\"torch.mm(a, b):\", torch.mm(a, b))\n",
    "print(\"a.matmul(b):\", a.matmul(b))\n",
    "print(\"a.mm(b):\", a.mm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622935d2",
   "metadata": {},
   "source": [
    "### 高维Tensor乘法——`matmul`\n",
    "- 不同于矩阵乘法，高维Tensor的乘法`matmul`不能被`@`、`mm`代替\n",
    "- 对于高维的Tensor(dim>2)，定义其矩阵乘法仅在最后的两个维度，要求前面的维度必须保持一致，就像矩阵的索引一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9885db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.matmul(a, b).shape: torch.Size([2, 3, 5])\n",
      "a.matmul(b).shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "b = torch.randn(2, 4, 5)\n",
    "print(\"torch.matmul(a, b).shape:\", torch.matmul(a, b).shape)\n",
    "print(\"a.matmul(b).shape:\", a.matmul(b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d85cfb",
   "metadata": {},
   "source": [
    "## 其他算术运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1706dc1",
   "metadata": {},
   "source": [
    "### 幂运算——`pow`\n",
    "- `pow`函数是对Tensor的每个元素进行幂运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d48aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.pow(a, 3): tensor([1, 8])\n",
      "a.pow(3): tensor([1, 8])\n",
      "a ** 3: tensor([1, 8])\n",
      "a.pow_(3): tensor([1, 8])\n",
      "a after a.pow_(3): tensor([1, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2])\n",
    "print(\"torch.pow(a, 3):\", torch.pow(a, 3))\n",
    "print(\"a.pow(3):\", a.pow(3))\n",
    "print(\"a ** 3:\", a ** 3)\n",
    "print(\"a.pow_(3):\", a.pow_(3))\n",
    "print(\"a after a.pow_(3):\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723a681",
   "metadata": {},
   "source": [
    "#### 自然指数运算——`exp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7da018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.type(): torch.FloatTensor\n",
      "torch.exp(a): tensor([2.7183, 7.3891])\n",
      "torch.exp_(a): tensor([2.7183, 7.3891])\n",
      "a after torch.exp_(a): tensor([2.7183, 7.3891])\n",
      "a.exp(): tensor([  15.1543, 1618.1781])\n",
      "a.exp_(): tensor([  15.1543, 1618.1781])\n",
      "a after a.exp_(): tensor([  15.1543, 1618.1781])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2], dtype=torch.float32)\n",
    "print(\"a.type():\", a.type())\n",
    "print(\"torch.exp(a):\", torch.exp(a))\n",
    "print(\"torch.exp_(a):\", torch.exp_(a))\n",
    "print(\"a after torch.exp_(a):\", a)\n",
    "print(\"a.exp():\", a.exp())\n",
    "print(\"a.exp_():\", a.exp_())\n",
    "print(\"a after a.exp_():\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5001cb",
   "metadata": {},
   "source": [
    "### 对数运算——`log`、`log2`、`log10`\n",
    "- `log`、`log2`、`log10`函数是对Tensor的每个元素进行底数为e、2、10的对数运算。\n",
    "- 张量类型最好是浮点数类型，否则会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b2f3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.log(a): tensor([2.3026, 0.6931])\n",
      "torch.log_(a): tensor([2.3026, 0.6931])\n",
      "a after torch.log_(a): tensor([2.3026, 0.6931])\n",
      "a.log(): tensor([ 0.8340, -0.3665])\n",
      "a.log_(): tensor([ 0.8340, -0.3665])\n",
      "a after a.log_(): tensor([ 0.8340, -0.3665])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([10, 2], dtype=torch.float32)  # 此处仅展示对数运算\n",
    "print(\"torch.log(a):\", torch.log(a))\n",
    "print(\"torch.log_(a):\", torch.log_(a))\n",
    "print(\"a after torch.log_(a):\", a)\n",
    "print(\"a.log():\", a.log())\n",
    "print(\"a.log_():\", a.log_())\n",
    "print(\"a after a.log_():\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6762bd",
   "metadata": {},
   "source": [
    "### 开方运算——`sqrt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ead6bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.sqrt(a): tensor([2., 3.])\n",
      "torch.sqrt_(a): tensor([2., 3.])\n",
      "a after torch.sqrt_(a): tensor([2., 3.])\n",
      "a.sqrt(): tensor([1.4142, 1.7321])\n",
      "a.sqrt_(): tensor([1.4142, 1.7321])\n",
      "a after a.sqrt_(): tensor([1.4142, 1.7321])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([4, 9], dtype=torch.float32)  # 对于平方根运算，输入需要是浮点数类型\n",
    "print(\"torch.sqrt(a):\", torch.sqrt(a))\n",
    "print(\"torch.sqrt_(a):\", torch.sqrt_(a))  # 若输入非浮点数类型，此处会报错\n",
    "print(\"a after torch.sqrt_(a):\", a)\n",
    "print(\"a.sqrt():\", a.sqrt())\n",
    "print(\"a.sqrt_():\", a.sqrt_())\n",
    "print(\"a after a.sqrt_():\", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
